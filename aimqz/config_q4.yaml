# config.yaml

model:
  path: "./Qwen2.5-7B-Instruct"

data:
  file: "./dataset/bedtimenews.jsonl"
  val_size: 0.1             # 10% 用于验证
  max_length: 4096          # 上下文长度

lora:
  r: 16                     # LoRA 秩，越大参数越多
  lora_alpha: 32            # 推荐为 r 的 2倍
  lora_dropout: 0.05
  target_modules:           # Qwen2.5 建议对所有线性层进行微调
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

training:
  output_dir: "./output_qwen_lora_q4"
  batch_size: 2             # 单卡 batch size，总 batch = 2 * 2 * grad_accum
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4     # LoRA 通常比全量微调大
  num_train_epochs: 5       # 数据集只有 2MB，可能需要多跑几轮，或者根据 loss 提前停止
  logging_steps: 10
  save_steps: 50
  eval_steps: 50
  warmup_ratio: 0.03
  fp16: False               # 使用 bf16 (40系显卡原生支持)
  bf16: True

quantization:
  use_4bit: True            # 启用 4-bit 量化 (QLoRA)
  bnb_4bit_compute_dtype: "bfloat16" # 4090 支持 bfloat16 计算，速度更快
  bnb_4bit_quant_type: "nf4" # 使用 NormalFloat 4 (NF4) 量化格式
  bnb_4bit_use_double_quant: True # 启用双重量化，进一步减少显存开销