import torch
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- é…ç½®å‚æ•° ---
# åŸå§‹é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
BASE_MODEL_PATH = "./Qwen2.5-7B-Instruct"
# è®­ç»ƒä¿å­˜çš„ LoRA é€‚é…å™¨è·¯å¾„ (ä¸ config.yaml ä¸­çš„ output_dir å¯¹åº”)
LORA_ADAPTER_PATH = "./output_qwen_lora"

# --- 1. åŠ è½½ Tokenizer ---
print("ğŸš€ æ­£åœ¨åŠ è½½ Tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# --- 2. åŠ è½½åŸºåº§æ¨¡å‹ ---
# æ³¨æ„ï¼šæ¨ç†é€šå¸¸åªç”¨å•å¡ï¼Œæ‰€ä»¥ä½¿ç”¨ device_map="auto" æˆ–æŒ‡å®šå…·ä½“çš„ "cuda:0"
# å¦‚æœä½ åœ¨è®­ç»ƒæ—¶ä½¿ç”¨äº† BF16ï¼Œè¿™é‡Œä¹Ÿå»ºè®®ä½¿ç”¨ BF16 æ¥åŠ è½½ï¼Œä»¥ä¿æŒç²¾åº¦ã€‚
print(f"ğŸ§  æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹ ({BASE_MODEL_PATH})...")
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH,
    torch_dtype=torch.bfloat16,  # ç¡®ä¿ä¸è®­ç»ƒæ—¶çš„ dtype ä¸€è‡´ (å¦‚æœæ˜¯ 4090 ä¸”è®­ç»ƒæ—¶ä½¿ç”¨äº† bf16)
    attn_implementation="flash_attention_2",  # æ¨ç†æ—¶ä¹Ÿä½¿ç”¨ Flash Attention 2 åŠ é€Ÿ
    device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ° GPU ä¸Š
    trust_remote_code=True
)

# --- 3. åŠ è½½ LoRA é€‚é…å™¨ ---
print(f"ğŸ§© æ­£åœ¨åŠ è½½ LoRA é€‚é…å™¨ ({LORA_ADAPTER_PATH})...")
model = PeftModel.from_pretrained(model, LORA_ADAPTER_PATH)

# --- 4. åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹ (å¯é€‰ä½†æ¨è) ---
# åˆå¹¶æƒé‡åï¼Œå¯ä»¥ç§»é™¤ PeftModel åŒ…è£…ï¼Œæé«˜æ¨ç†é€Ÿåº¦å¹¶èŠ‚çœæ˜¾å­˜ã€‚
print("âœ¨ æ­£åœ¨åˆå¹¶ LoRA æƒé‡...")
# å¿…é¡»å°†æ¨¡å‹åˆ‡æ¢åˆ° eval æ¨¡å¼
model.eval()
model = model.merge_and_unload()  # åˆå¹¶å¹¶å¸è½½ LoRA ç»“æ„


# --- 5. å¯¹è¯æ¨ç†å¾ªç¯ ---
def chat_loop():
    print("\n" + "=" * 50)
    print("å¼€å§‹å¯¹è¯ (è¾“å…¥ 'exit' é€€å‡º, 'clear' æ¸…ç©ºå†å²)")
    print("=" * 50)

    history = []

    while True:
        try:
            user_input = input("æˆ‘: ")
            if user_input.lower() in ['exit', 'quit']:
                print("å¯¹è¯ç»“æŸã€‚")
                break
            if user_input.lower() == 'clear':
                history = []
                print("å†å²è®°å½•å·²æ¸…ç©ºã€‚")
                continue

            # æ„é€ å¯¹è¯å†å²
            history.append({"role": "user", "content": user_input})

            # ä½¿ç”¨ tokenizer.apply_chat_template å‡†å¤‡è¾“å…¥
            input_text = tokenizer.apply_chat_template(
                history,
                tokenize=False,
                add_generation_prompt=True  # è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼Œå‘Šè¯‰æ¨¡å‹æ¥ä¸‹æ¥åº”è¯¥ç”Ÿæˆ assistant çš„å›å¤
            )

            # Tokenize å’Œç§»è‡³è®¾å¤‡
            input_ids = tokenizer(input_text, return_tensors="pt").input_ids
            # ç¡®ä¿ input_ids åœ¨ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ä¸Š (é€šå¸¸æ˜¯ CUDA)
            input_ids = input_ids.to(model.device)

            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=input_ids,
                    max_new_tokens=2048,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )

            # è§£ç å›å¤ï¼Œè·³è¿‡è¾“å…¥éƒ¨åˆ†
            response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)

            print(f"é©¬ç£å·¥: {response}")
            print("\n")

            # å°†æ¨¡å‹çš„å›å¤æ·»åŠ åˆ°å†å²è®°å½•ä¸­
            history.append({"role": "assistant", "content": response})

        except EOFError:
            break
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {e}")
            break


if __name__ == "__main__":
    chat_loop()
